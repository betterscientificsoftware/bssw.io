# Advanced Training on Extreme-Scale Computing through ATPESC

#### Contributed by [Yasaman Ghadar](https://github.com/yghadar) and [Ray Loy](https://github.com/rloy)

#### Publication date: January 27, 2021

**Hero Image:**
 
- <img src='../../images/Blog_0121_ATPESC.jpg'>

High-performance computing (HPC) education is essential to the broader computational community. From evolving programming techniques and numerical algorithms to transformations in system architectures and software, the HPC landscape is moving fast as it continues progressing toward the exascale era. But is the HPC community prepared?

### Addressing a gap in professional training
Using supercomputers for computational science and engineering (CSE) research requires expertise that is not always covered by formal education. As a means to fill the gap that exists in the training computational scientists typically receive through formal education or other shorter courses, we created the [Argonne Training Program on Extreme-Scale Computing (ATPESC)](https://extremecomputingtraining.anl.gov) in 2013. This intensive, two-week course provides training on the key skills, approaches, and tools to design, implement, and execute computational science and engineering applications on current high-end computing systems and the leadership-class computing systems of the future. 

ATPESC participants are provided access to some of today’s most powerful supercomputing resources, including the Argonne Leadership Computing Facility’s systems, the Oak Ridge Leadership Computing Facility’s systems, the National Energy Research Scientific Computing Center’s systems, and leading-edge testbeds from Argonne’s Joint Laboratory for System Evaluation.

While only approximately 70 participants are able to attend ATPESC each year, the entire HPC community can tap into the program’s broad curriculum via the [Argonne YouTube Training Channel](http://extremecomputingtraining.anl.gov/agenda-2020). In an effort to extend the reach of ATPESC beyond the classroom, program organizers have compiled playlists featuring four years' worth of past lectures from some of the world’s foremost experts and pioneers in extreme-scale computing. 

ATPESC is funded by the Exascale Computing Project, a collaborative effort of the U.S. Department of Energy Office of Science’s Advanced Scientific Computing Research Program and the National Nuclear Security Administration.

### Apply for this year's training
In the summer of 2021, ATPESC will be back for its ninth year, providing a new group of early career researchers with an opportunity to learn and improve their skills to use extreme-scale computing systems for science. The event will be held in the Chicago area; however, if an in-person meeting is not possible, it will be held as a virtual event.  If you or someone you know may be interested in attending, visit the [ATPESC website](https://extremecomputingtraining.anl.gov) for details on the open call for applications. **The deadline to apply is Monday March 1, 2021.**

### Author bios
Yasaman Ghadar is the ATPESC Deputy Program Director and an assistant computational scientist in the performance engineering group at the Argonne Leadership Computing Facility (ALCF). Her research focuses on porting applications such as molecular dynamic simulation software packages to Aurora, with emphasis on the OpenMP and Kokkos programming models. She is also part of the ALCF training team who design and organize workshops and seminars to help users across the globe to take advantge of the leadership computing facility. 

Ray Loy is the ATPESC Program Director and the Lead for Training, Debugging, and Math Libraries at ALCF. Among his interests are performance optimization, parallel debuggers, parallel I/O, transactional memory, and the application/queuing system interface.  


<!---
Publish: yes
Track: community
RSS update: 2021-01-27
Categories: performance
Topics: high-performance computing (HPC), performance at leadership computing facilities
Tags: bssw-blog-article
Level: 2
Prerequisites: default
Aggregate: none
--->
